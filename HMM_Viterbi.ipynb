{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMM_Viterbi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVepPJHjqypj6FocS39ncG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikram-hamizi/AML-DS-2021/blob/main/HMM_Viterbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aBuDAWk0Nr5"
      },
      "source": [
        "### **Hidden Markov Models and Deep Learning for POS Tagging**\n",
        "\n",
        "- HMM: Viterbi Algorithm\n",
        "- Deep Learning (with convolutions)\n",
        "- Deep Learning (Recurrent NN)\n",
        "\n",
        "Task Link: [click](https://hackmd.io/@gFZmdMTOQxGFHEFqqU8pMQ/SyYIjrBU_#Hidden-Markov-Models-for-POS-Tagging)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1XIZHiHnydN",
        "outputId": "83ca505e-a9f4-4bfc-95a4-f33274bb22aa"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cP2WWgVovFK"
      },
      "source": [
        "# Make Resutls Reproducible\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM5SAG4m0DFj"
      },
      "source": [
        "### Load the Data + Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OoU6u50h6A-",
        "outputId": "2da8a93f-2c3e-49bd-b46c-aed37330d844"
      },
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "import wget\n",
        "\n",
        "wget.download('https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt')\n",
        "wget.download('https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt')\n",
        "\n",
        "names = ['Word', 'Tag']\n",
        "train = pd.read_csv(\"train_pos.txt\", sep=\" \", names=names) #a modified conll2000 dataset\n",
        "test = pd.read_csv(\"test_pos.txt\", sep=\" \", names=names)\n",
        "\n",
        "train = train.astype(str)\n",
        "test = test.astype(str)\n",
        "\n",
        "train.shape, test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((220663, 2), (49389, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "K5qeOaQ7k1tL",
        "outputId": "f0491f46-9918-40e5-fa73-47998d9d4b4a"
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Confidence</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pound</td>\n",
              "      <td>NN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>is</td>\n",
              "      <td>VBZ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Word  Tag\n",
              "0  Confidence   NN\n",
              "1          in   IN\n",
              "2         the   DT\n",
              "3       pound   NN\n",
              "4          is  VBZ"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVkFqnG68Vx-",
        "outputId": "9ad18642-2503-4ac7-ce18-fc157d7b4a3a"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "train_y_enc = label_encoder.fit_transform(train['Tag'])\n",
        "\n",
        "tags_index = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))) #INDEX {tag:int} #starts from 1\n",
        "tags_index_size = len(tags_index) \n",
        "\"Number of unique tags:\", tags_index_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Number of unique tags:', 45)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYbmSDrWKY5Y",
        "outputId": "deba3be8-316f-491a-e7b4-3692039e1ee7"
      },
      "source": [
        "# sanity check: are they the same order? -> yes\n",
        "print(label_encoder.classes_)\n",
        "print(np.array(list(tags_index.keys())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#' '$' \"''\" '(' ')' ',' '.' ':' 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR'\n",
            " 'JJS' 'MD' 'NN' 'NNP' 'NNPS' 'NNS' 'PDT' 'POS' 'PRP' 'PRP$' 'RB' 'RBR'\n",
            " 'RBS' 'RP' 'SYM' 'TO' 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ' 'WDT' 'WP'\n",
            " 'WP$' 'WRB' '``' 'nan']\n",
            "['#' '$' \"''\" '(' ')' ',' '.' ':' 'CC' 'CD' 'DT' 'EX' 'FW' 'IN' 'JJ' 'JJR'\n",
            " 'JJS' 'MD' 'NN' 'NNP' 'NNPS' 'NNS' 'PDT' 'POS' 'PRP' 'PRP$' 'RB' 'RBR'\n",
            " 'RBS' 'RP' 'SYM' 'TO' 'UH' 'VB' 'VBD' 'VBG' 'VBN' 'VBP' 'VBZ' 'WDT' 'WP'\n",
            " 'WP$' 'WRB' '``' 'nan']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "KLPIQYTtpBYJ",
        "outputId": "09740e64-9e2d-4336-9d8b-aed56b0899b2"
      },
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.countplot(train['Tag'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0262209310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ui8Jj-ym8Fb"
      },
      "source": [
        "import csv\n",
        "from keras.utils import np_utils \n",
        "\n",
        "def sentence_tags_to_vector(sentence_tags, tags_index):\n",
        "  vector = []\n",
        "  for tag in sentence_tags:\n",
        "    if tag in tags_index:\n",
        "      vector.append(tags_index[tag])\n",
        "    else:\n",
        "      vector.append(-999)\n",
        "  return vector\n",
        "\n",
        "#Function: transforms dataset to a table of sentences and their corresebonding sequences of tags\n",
        "def transform_to_sentence_tags_table(file_path, numerical_labels=False, ohe_labels=False, sen_maxlen=100):\n",
        "\n",
        "  sentences_list = []\n",
        "  tags_list = []\n",
        "\n",
        "  sentence = []\n",
        "  sentence_tags = []\n",
        "\n",
        "  i=0\n",
        "  with open(file_path, \"r\") as file:\n",
        "    csv_reader = csv.reader(file, delimiter=' ')\n",
        "\n",
        "    for line in csv_reader:\n",
        "      i+=1  \n",
        "      if len(line[0])==0 or line[0]==\" \":\n",
        "        \n",
        "        if i>1:\n",
        "          #Create sentence\n",
        "          sentences_list.append(sentence)\n",
        "\n",
        "          #Create list of tags\n",
        "\n",
        "          if ohe_labels or numerical_labels:\n",
        "            numerical_vector = sentence_tags_to_vector(sentence_tags, tags_index)\n",
        "            if ohe_labels:\n",
        "              padded_vector = pad_sequences([numerical_vector], padding='post', maxlen=sen_maxlen)\n",
        "              ohe_matrix = np_utils.to_categorical(padded_vector, num_classes=len(tags_index), dtype=np.float64)\n",
        "              tags_list.append(ohe_matrix)\n",
        "            elif numerical_labels:\n",
        "              padded_vector = pad_sequences([numerical_vector], padding='post', maxlen=len(tags_index))\n",
        "              tags_list.append(padded_vector)\n",
        "          elif not ohe_labels and not numerical_labels:\n",
        "            tags_list.append(sentence_tags)\n",
        "\n",
        "        #Reset\n",
        "        sentence = []\n",
        "        sentence_tags = []\n",
        "\n",
        "      else:\n",
        "        sentence.append(line[0])\n",
        "        sentence_tags.append(line[1])\n",
        "\n",
        "  df_sentences = pd.DataFrame(list(zip(sentences_list, tags_list)), columns=['Sentence', 'Tag'])\n",
        "  return df_sentences\n",
        "\n",
        "train_sentences = transform_to_sentence_tags_table('train_pos.txt')\n",
        "test_sentences = transform_to_sentence_tags_table('test_pos.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTpsz7Yox_Go",
        "outputId": "ff9c2ad4-939a-4b62-e43f-2b924e4e6ceb"
      },
      "source": [
        "print(\"Number of sentences:\", train_sentences.shape) #number of sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: (8936, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "npuxEhsQzWXs",
        "outputId": "93680f52-8231-4dcb-abd9-09ce319274ac"
      },
      "source": [
        "train_sentences.head(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Confidence, in, the, pound, is, widely, expec...</td>\n",
              "      <td>[NN, IN, DT, NN, VBZ, RB, VBN, TO, VB, DT, JJ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Chancellor, of, the, Exchequer, Nigel, Lawson...</td>\n",
              "      <td>[NNP, IN, DT, NNP, NNP, NNP, POS, VBN, NN, TO,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[But, analysts, reckon, underlying, support, f...</td>\n",
              "      <td>[CC, NNS, VBP, VBG, NN, IN, NN, VBZ, VBN, VBN,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[This, has, increased, the, risk, of, the, gov...</td>\n",
              "      <td>[DT, VBZ, VBN, DT, NN, IN, DT, NN, VBG, VBN, T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[``, The, risks, for, sterling, of, a, bad, tr...</td>\n",
              "      <td>[``, DT, NNS, IN, NN, IN, DT, JJ, NN, NN, VBP,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[``, If, there, is, another, bad, trade, numbe...</td>\n",
              "      <td>[``, IN, EX, VBZ, DT, JJ, NN, NN, ,, EX, MD, V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence                                                Tag\n",
              "0  [Confidence, in, the, pound, is, widely, expec...  [NN, IN, DT, NN, VBZ, RB, VBN, TO, VB, DT, JJ,...\n",
              "1  [Chancellor, of, the, Exchequer, Nigel, Lawson...  [NNP, IN, DT, NNP, NNP, NNP, POS, VBN, NN, TO,...\n",
              "2  [But, analysts, reckon, underlying, support, f...  [CC, NNS, VBP, VBG, NN, IN, NN, VBZ, VBN, VBN,...\n",
              "3  [This, has, increased, the, risk, of, the, gov...  [DT, VBZ, VBN, DT, NN, IN, DT, NN, VBG, VBN, T...\n",
              "4  [``, The, risks, for, sterling, of, a, bad, tr...  [``, DT, NNS, IN, NN, IN, DT, JJ, NN, NN, VBP,...\n",
              "5  [``, If, there, is, another, bad, trade, numbe...  [``, IN, EX, VBZ, DT, JJ, NN, NN, ,, EX, MD, V..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6664IUrrNZP"
      },
      "source": [
        "# 0. split to x and y\n",
        "train_x = train['Word']\n",
        "train_y = train['Tag'].to_numpy()\n",
        "\n",
        "test_x = test['Word']\n",
        "test_y = test['Tag'].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBwGwMpxWGSl"
      },
      "source": [
        "unique_tags = train['Tag'].unique()\n",
        "unique_words = train['Word'].unique()\n",
        "\n",
        "# numeric variables\n",
        "total_words = len(unique_words)\n",
        "total_tags = len(unique_tags) #len(unique_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diz7qn6LmHpN"
      },
      "source": [
        "### **Task 1**: Calculate the transition probability and emission matrices \n",
        "This is the First step towards viterbi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x29fowna1_O0"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "agg_dict = {'Tag':Counter}\n",
        "train_agg = train.groupby(by=['Word'], as_index=False).agg(agg_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKx1qJYRWnCH"
      },
      "source": [
        "from tqdm import tqdm \n",
        "\n",
        "\"\"\"1. Func: get prior probabilities\"\"\"\n",
        "def get_priors():\n",
        "  priors = np.empty(total_tags) #vector\n",
        "\n",
        "  counts = train['Tag'].value_counts()\n",
        "  total = sum(counts) # = 220663 = len(train)\n",
        "\n",
        "  priors = counts/total\n",
        "  return priors\n",
        "\n",
        "\n",
        "\"\"\"2. Func: get EMISSIONS probabilities matrix\"\"\"\n",
        "\n",
        "#helper function\n",
        "def get_emission_proba_vector(word):\n",
        "  #1. GET COUNTS\n",
        "  tags_counts = dict.fromkeys(unique_tags.tolist(),1e-8) #initialize dict with keys (unique tags)\n",
        "  word_sample_space_count = 0\n",
        "\n",
        "  #Join the two columns by word. The tag column becomes a counter of tags \n",
        "  tag_counter_of_word = list(train_agg[train_agg['Word']==word].to_numpy()[0][1].items())\n",
        "  for tag_counter in tag_counter_of_word:\n",
        "    tags_counts[tag_counter[0]] += tag_counter[1]\n",
        "    word_sample_space_count += tag_counter[1]\n",
        "\n",
        "  #2. GET PROBAS\n",
        "  word_probs_vector = list(tags_counts.values())\n",
        "  if word_sample_space_count is not 0:\n",
        "    word_probs_vector = [w/word_sample_space_count for w in word_probs_vector]\n",
        "\n",
        "  return word_probs_vector\n",
        "\n",
        "#EMISSIONS\n",
        "def get_emissions_matrix():\n",
        "  emissions = np.empty((total_words, total_tags)) #matrix'\n",
        "\n",
        "  with tqdm(total=total_words) as pbar:\n",
        "    for i, word in enumerate(unique_words):\n",
        "      emissions[i] = get_emission_proba_vector(word) # P(ti|tj)\n",
        "      pbar.update()\n",
        "\n",
        "  return emissions\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"3. Func: get transition probabilities matrix\"\"\"\n",
        "\n",
        "#helper function\n",
        "def get_trans_proba_vector(tag):\n",
        "\n",
        "  tags_counts = dict.fromkeys(unique_tags.tolist(),1e-8) #initialize dict with keys (unique tags)\n",
        "  tags_rows = train_sentences['Tag']\n",
        "\n",
        "  #1. GET COUNTS\n",
        "  for list_of_tags in tags_rows: #for each list of tags in a row\n",
        "\n",
        "    if tag not in list_of_tags: continue;  \n",
        "    else: i = list_of_tags.index(tag) #index of our tag\n",
        "    \n",
        "    for iter in range(len(list_of_tags)):#for each tag present in the row\n",
        "      if i+1>=len(list_of_tags): break;\n",
        "\n",
        "      next_tag = list_of_tags[i+1:][0] #get the tag that appears after our desired tag\n",
        "      if 0>=i+1>=len(list_of_tags): break\n",
        "\n",
        "      if (list_of_tags[i]==tag): #if the previous tag (i) is our tag, increment the count of our current one\n",
        "        tags_counts[next_tag]+=1\n",
        "\n",
        "      if tag not in list_of_tags[i+1:]: break;\n",
        "      else: i += list_of_tags[i+1:].index(tag)+1; \n",
        "\n",
        "  tags_probs_vector = list(tags_counts.values())\n",
        "  \n",
        "  #2. GET PROBAS\n",
        "  tag_sample_space_count =  sum(tags_probs_vector)\n",
        "  tags_probs_vector = [t/tag_sample_space_count for t in tags_probs_vector] if tag_sample_space_count is not 0 else tags_probs_vector\n",
        "\n",
        "  return tags_probs_vector\n",
        "\n",
        "#TRANSITION\n",
        "def get_transitions_matrix():\n",
        "  transitions = np.zeros((total_tags, total_tags), dtype='float32') #matrix\n",
        "\n",
        "  with tqdm(total=total_tags) as pbar:\n",
        "    for i, tag in enumerate(unique_tags):\n",
        "        transitions[i] = get_trans_proba_vector(tag) # P(ti|tj)\n",
        "        pbar.update()\n",
        "\n",
        "  return transitions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFmzqM_xAUTK",
        "outputId": "06b0ebb0-8ffe-4533-f085-fb3c6dbfcef6"
      },
      "source": [
        "#1. Prior Probabilities Pi: P(ti)\n",
        "priors = get_priors() #vector\n",
        "\n",
        "#2. Emission Probabilities: P(wi|ti)\n",
        "emissions = get_emissions_matrix() #matrix\n",
        "\n",
        "#3. Transision Probabilities: P(ti|ti-1)\n",
        "transitions = get_transitions_matrix() #matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19123/19123 [00:36<00:00, 517.11it/s]\n",
            "100%|██████████| 45/45 [00:00<00:00, 67.20it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "KMoaLwPUdKeH",
        "outputId": "0f0c9971-6a12-4ad3-88d1-a31725f64b4b"
      },
      "source": [
        "#convert the two matrices to dataframes\n",
        "emissions_df = pd.DataFrame(emissions, columns=unique_tags, index=unique_words)\n",
        "transitions_df = pd.DataFrame(transitions, columns=unique_tags, index=unique_tags)\n",
        "transitions_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NN</th>\n",
              "      <th>IN</th>\n",
              "      <th>DT</th>\n",
              "      <th>VBZ</th>\n",
              "      <th>RB</th>\n",
              "      <th>VBN</th>\n",
              "      <th>TO</th>\n",
              "      <th>VB</th>\n",
              "      <th>JJ</th>\n",
              "      <th>NNS</th>\n",
              "      <th>NNP</th>\n",
              "      <th>,</th>\n",
              "      <th>CC</th>\n",
              "      <th>POS</th>\n",
              "      <th>.</th>\n",
              "      <th>nan</th>\n",
              "      <th>VBP</th>\n",
              "      <th>VBG</th>\n",
              "      <th>PRP$</th>\n",
              "      <th>CD</th>\n",
              "      <th>``</th>\n",
              "      <th>''</th>\n",
              "      <th>VBD</th>\n",
              "      <th>EX</th>\n",
              "      <th>MD</th>\n",
              "      <th>#</th>\n",
              "      <th>(</th>\n",
              "      <th>$</th>\n",
              "      <th>)</th>\n",
              "      <th>NNPS</th>\n",
              "      <th>PRP</th>\n",
              "      <th>JJS</th>\n",
              "      <th>WP</th>\n",
              "      <th>RBR</th>\n",
              "      <th>JJR</th>\n",
              "      <th>WDT</th>\n",
              "      <th>WRB</th>\n",
              "      <th>RBS</th>\n",
              "      <th>PDT</th>\n",
              "      <th>RP</th>\n",
              "      <th>:</th>\n",
              "      <th>FW</th>\n",
              "      <th>WP$</th>\n",
              "      <th>SYM</th>\n",
              "      <th>UH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NN</th>\n",
              "      <td>0.117678</td>\n",
              "      <td>0.247835</td>\n",
              "      <td>0.007699</td>\n",
              "      <td>0.038795</td>\n",
              "      <td>0.017058</td>\n",
              "      <td>0.012080</td>\n",
              "      <td>0.041483</td>\n",
              "      <td>0.001427</td>\n",
              "      <td>0.009956</td>\n",
              "      <td>0.084990</td>\n",
              "      <td>0.009724</td>\n",
              "      <td>0.112534</td>\n",
              "      <td>0.039724</td>\n",
              "      <td>2.203564e-02</td>\n",
              "      <td>0.105731</td>\n",
              "      <td>3.318621e-13</td>\n",
              "      <td>0.003650</td>\n",
              "      <td>0.007898</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.005343</td>\n",
              "      <td>0.002190</td>\n",
              "      <td>5.210235e-03</td>\n",
              "      <td>0.048651</td>\n",
              "      <td>3.318621e-04</td>\n",
              "      <td>0.018186</td>\n",
              "      <td>3.318621e-05</td>\n",
              "      <td>0.001792</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>2.024359e-03</td>\n",
              "      <td>2.986759e-04</td>\n",
              "      <td>0.005277</td>\n",
              "      <td>3.318621e-13</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>0.003850</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.007865</td>\n",
              "      <td>1.991173e-03</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>3.318621e-05</td>\n",
              "      <td>6.637241e-05</td>\n",
              "      <td>0.011482</td>\n",
              "      <td>1.659310e-04</td>\n",
              "      <td>1.991172e-04</td>\n",
              "      <td>6.637241e-05</td>\n",
              "      <td>3.318621e-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IN</th>\n",
              "      <td>0.111560</td>\n",
              "      <td>0.029746</td>\n",
              "      <td>0.326245</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.014983</td>\n",
              "      <td>0.005844</td>\n",
              "      <td>0.003427</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.084186</td>\n",
              "      <td>0.061470</td>\n",
              "      <td>0.136737</td>\n",
              "      <td>0.004482</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>4.393866e-13</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>4.393866e-13</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.027725</td>\n",
              "      <td>0.036689</td>\n",
              "      <td>0.062129</td>\n",
              "      <td>0.006064</td>\n",
              "      <td>5.272640e-04</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>1.449976e-03</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>7.030186e-04</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.027857</td>\n",
              "      <td>4.393866e-05</td>\n",
              "      <td>1.889362e-03</td>\n",
              "      <td>0.030801</td>\n",
              "      <td>3.031768e-03</td>\n",
              "      <td>0.002417</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>0.005361</td>\n",
              "      <td>0.003251</td>\n",
              "      <td>1.713608e-03</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>1.230282e-03</td>\n",
              "      <td>4.393866e-13</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>2.196933e-04</td>\n",
              "      <td>4.393866e-13</td>\n",
              "      <td>4.393866e-13</td>\n",
              "      <td>4.393866e-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DT</th>\n",
              "      <td>0.484591</td>\n",
              "      <td>0.008237</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.008891</td>\n",
              "      <td>0.010909</td>\n",
              "      <td>0.015600</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.198931</td>\n",
              "      <td>0.073201</td>\n",
              "      <td>0.111002</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.010091</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.025746</td>\n",
              "      <td>0.004909</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>0.003218</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>3.272787e-04</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.008455</td>\n",
              "      <td>1.090929e-04</td>\n",
              "      <td>2.127311e-03</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>9.327442e-03</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.001364</td>\n",
              "      <td>0.006382</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>0.003055</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>1.636393e-04</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>5.454644e-13</td>\n",
              "      <td>2.181858e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VBZ</th>\n",
              "      <td>0.039595</td>\n",
              "      <td>0.095976</td>\n",
              "      <td>0.168496</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.134495</td>\n",
              "      <td>0.144609</td>\n",
              "      <td>0.045406</td>\n",
              "      <td>0.002582</td>\n",
              "      <td>0.075963</td>\n",
              "      <td>0.017215</td>\n",
              "      <td>0.061760</td>\n",
              "      <td>0.027975</td>\n",
              "      <td>0.003658</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>0.029051</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>0.001291</td>\n",
              "      <td>0.051216</td>\n",
              "      <td>0.010114</td>\n",
              "      <td>0.017861</td>\n",
              "      <td>0.008393</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>0.003443</td>\n",
              "      <td>8.607704e-04</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.004519</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>0.022380</td>\n",
              "      <td>4.303852e-04</td>\n",
              "      <td>0.002582</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>0.007962</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>7.746933e-03</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>2.151926e-04</td>\n",
              "      <td>1.506348e-03</td>\n",
              "      <td>0.004949</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>2.151926e-12</td>\n",
              "      <td>2.151926e-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RB</th>\n",
              "      <td>0.008023</td>\n",
              "      <td>0.129276</td>\n",
              "      <td>0.050711</td>\n",
              "      <td>0.041023</td>\n",
              "      <td>0.058886</td>\n",
              "      <td>0.086285</td>\n",
              "      <td>0.025431</td>\n",
              "      <td>0.109749</td>\n",
              "      <td>0.102785</td>\n",
              "      <td>0.004995</td>\n",
              "      <td>0.006812</td>\n",
              "      <td>0.093249</td>\n",
              "      <td>0.007115</td>\n",
              "      <td>4.541326e-04</td>\n",
              "      <td>0.062519</td>\n",
              "      <td>1.513775e-12</td>\n",
              "      <td>0.031184</td>\n",
              "      <td>0.030730</td>\n",
              "      <td>0.002725</td>\n",
              "      <td>0.022858</td>\n",
              "      <td>0.001060</td>\n",
              "      <td>1.059643e-03</td>\n",
              "      <td>0.050560</td>\n",
              "      <td>6.055101e-04</td>\n",
              "      <td>0.011505</td>\n",
              "      <td>3.027551e-04</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.015289</td>\n",
              "      <td>1.513775e-04</td>\n",
              "      <td>3.027551e-04</td>\n",
              "      <td>0.011202</td>\n",
              "      <td>6.055101e-04</td>\n",
              "      <td>0.001665</td>\n",
              "      <td>0.004693</td>\n",
              "      <td>0.017408</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>3.481683e-03</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>4.541326e-04</td>\n",
              "      <td>1.513775e-12</td>\n",
              "      <td>0.003784</td>\n",
              "      <td>1.513775e-04</td>\n",
              "      <td>1.513775e-12</td>\n",
              "      <td>1.513775e-12</td>\n",
              "      <td>1.513775e-12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           NN        IN        DT  ...           WP$           SYM            UH\n",
              "NN   0.117678  0.247835  0.007699  ...  1.991172e-04  6.637241e-05  3.318621e-13\n",
              "IN   0.111560  0.029746  0.326245  ...  4.393866e-13  4.393866e-13  4.393866e-13\n",
              "DT   0.484591  0.008237  0.001309  ...  5.454644e-13  5.454644e-13  2.181858e-04\n",
              "VBZ  0.039595  0.095976  0.168496  ...  2.151926e-12  2.151926e-12  2.151926e-12\n",
              "RB   0.008023  0.129276  0.050711  ...  1.513775e-12  1.513775e-12  1.513775e-12\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "I-jlztJffKPz",
        "outputId": "6cc370a5-0789-4175-9b30-2a4ad66dcdbd"
      },
      "source": [
        "emissions_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NN</th>\n",
              "      <th>IN</th>\n",
              "      <th>DT</th>\n",
              "      <th>VBZ</th>\n",
              "      <th>RB</th>\n",
              "      <th>VBN</th>\n",
              "      <th>TO</th>\n",
              "      <th>VB</th>\n",
              "      <th>JJ</th>\n",
              "      <th>NNS</th>\n",
              "      <th>NNP</th>\n",
              "      <th>,</th>\n",
              "      <th>CC</th>\n",
              "      <th>POS</th>\n",
              "      <th>.</th>\n",
              "      <th>nan</th>\n",
              "      <th>VBP</th>\n",
              "      <th>VBG</th>\n",
              "      <th>PRP$</th>\n",
              "      <th>CD</th>\n",
              "      <th>``</th>\n",
              "      <th>''</th>\n",
              "      <th>VBD</th>\n",
              "      <th>EX</th>\n",
              "      <th>MD</th>\n",
              "      <th>#</th>\n",
              "      <th>(</th>\n",
              "      <th>$</th>\n",
              "      <th>)</th>\n",
              "      <th>NNPS</th>\n",
              "      <th>PRP</th>\n",
              "      <th>JJS</th>\n",
              "      <th>WP</th>\n",
              "      <th>RBR</th>\n",
              "      <th>JJR</th>\n",
              "      <th>WDT</th>\n",
              "      <th>WRB</th>\n",
              "      <th>RBS</th>\n",
              "      <th>PDT</th>\n",
              "      <th>RP</th>\n",
              "      <th>:</th>\n",
              "      <th>FW</th>\n",
              "      <th>WP$</th>\n",
              "      <th>SYM</th>\n",
              "      <th>UH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Confidence</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "      <td>1.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>4.414361e-03</td>\n",
              "      <td>9.935256e-01</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.060035e-03</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "      <td>2.942908e-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.844018e-03</td>\n",
              "      <td>9.981560e-01</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "      <td>1.084716e-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pound</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "      <td>7.142857e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>9.978799e-01</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>2.120141e-03</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "      <td>7.067138e-12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      NN            IN  ...           SYM            UH\n",
              "Confidence  1.000000e+00  1.000000e-08  ...  1.000000e-08  1.000000e-08\n",
              "in          4.414361e-03  9.935256e-01  ...  2.942908e-12  2.942908e-12\n",
              "the         1.084716e-12  1.844018e-03  ...  1.084716e-12  1.084716e-12\n",
              "pound       1.000000e+00  7.142857e-10  ...  7.142857e-10  7.142857e-10\n",
              "is          7.067138e-12  7.067138e-12  ...  7.067138e-12  7.067138e-12\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oKlH_ypmWTB"
      },
      "source": [
        "### **Task 2**: Implement Viterbi Algorithm for POS-Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI2Y8Ij7ASfo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def viterbi(sentence, emissions_df, transitions_df, priors):\n",
        "  \n",
        "  number_of_words = len(sentence)\n",
        "  total_tags = len(transitions)\n",
        "  \n",
        "  highest_prob_so_far = 1\n",
        "\n",
        "  best_states_vector = np.zeros(number_of_words, dtype=object) #states are strings (tags)\n",
        "  viterbi_matrix = np.zeros((total_tags, number_of_words))\n",
        "\n",
        "  for i, word in enumerate(sentence):\n",
        "    # multiply: highest_prob_so_far*p(wi|tags)*p(previous_tag); i.e.\n",
        "    # multiply: highest_prob_so_far_value * emissions(word)_vector * transitions(previous_tag)_vector\n",
        "    \n",
        "    if word not in emissions_df.index:\n",
        "      word = 'nan'\n",
        "\n",
        "    emissions_word = emissions_df.loc[word,:].to_numpy()\n",
        "    \n",
        "    if i==0: #initialize: use priors instead of highest_prob_so_far\n",
        "      viterbi_matrix[:,i] = np.log10(priors) + np.log10(emissions_word)\n",
        "    else:\n",
        "      transitions_tag = transitions_df.loc[best_states_vector[i-1],:].to_numpy()\n",
        "      viterbi_matrix[:,i] = np.log10(highest_prob_so_far)+np.log10(emissions_word)+np.log10(transitions_tag)\n",
        "      \n",
        "    highest_prob_so_far = pow(10, max(viterbi_matrix[:,i]))\n",
        "    highest_prob_so_far_index = np.argmax(viterbi_matrix[:,i]) #index = tag_number_representation\n",
        "    best_states_vector[i] = unique_tags[highest_prob_so_far_index] \n",
        "\n",
        "  return best_states_vector #most likely path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcH7LVtnFpwJ"
      },
      "source": [
        "def viterbi_predict(test_x):\n",
        "  pred_y = []\n",
        "\n",
        "  with tqdm(total=len(test_x)) as pbar:\n",
        "    for sentence in test_x:\n",
        "      pred_y.append(viterbi(sentence, emissions_df, transitions_df, priors))\n",
        "      pbar.update()\n",
        "\n",
        "  return pred_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1fmXuerRB8h"
      },
      "source": [
        "def accuracy(y_test, y_pred):\n",
        "  avg_accuracy = 0\n",
        "  for test_instance, pred_instance in list(zip(y_test, y_pred)):\n",
        "    correct = [y1 for y1, y2 in list(zip(test_instance, pred_instance)) if y1 == y2] \n",
        "    avg_accuracy += len(correct)/float(len(test_instance))/float(len(y_test))\n",
        "    \n",
        "  return avg_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHv9l95gIE_x",
        "outputId": "d1200111-dfee-436e-b297-1894b4d73ab5"
      },
      "source": [
        "test_x_viterbi = test_sentences['Sentence'].to_numpy()\n",
        "test_y_viterbi = test_sentences['Tag'].to_numpy()\n",
        "\n",
        "start_viterbi = time()\n",
        "pred_y_viterbi = viterbi_predict(test_x_viterbi)\n",
        "end_viterbi = time()\n",
        "total_viterbi = end_viterbi - start_viterbi\n",
        "\n",
        "print(f\"\\n1. Viterbi Accuracy = {accuracy(test_y_viterbi, pred_y_viterbi)}%\")\n",
        "print(f\"Time to finish: {total_viterbi} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2012/2012 [00:15<00:00, 127.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1. Viterbi Accuracy = 0.8994715519094878%\n",
            "Time to finish: 15.727505683898926 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45mI1-ucmOZh"
      },
      "source": [
        "### **Task 3**: Implement RNN for POS-Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyKThe3WZH_T"
      },
      "source": [
        "#### **1. Convert all words to numerical vector representation**\n",
        "1. Convert each **word to an integer** (index). Start from 1. Reserve 2 for `PAD` and `UNK`.\n",
        "2. Create a **matrix of sentences** where each cell is an integer representings word. Each sentence is of legnth 100 (100 words). Pad the sentences. This matrix can be fed to the embedding layer.\n",
        "3. Set the **embedding layer** where the rows indices correspond to the word integers.\n",
        "\n",
        "***Note***: *in the dataset, the maximum sentence length is 73, and the maximum word length is 43.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Mv1pbhbSuS"
      },
      "source": [
        "'''1. VOCAB INDEX: Convert each **word to an integer** (index). Start from 1. Reserve 2 for PAD and UNK.'''\n",
        "unique_words = train['Word'].unique()\n",
        "unique_words_lower = np.unique((np.array([w.lower() for w in unique_words])))\n",
        "vocab = dict(zip(unique_words_lower, range(1,len(unique_words_lower)+1))) #INDEX {character:int} #starts from 1\n",
        "vocab_size = len(vocab) # = 17259 (when it is not lowered, it is 19123 == len(unique_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPAbcLtF1_Wa"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "#a. Find the maximum length of a sentence\n",
        "sen = train_sentences['Sentence'].to_numpy()\n",
        "sen_maxlen = len(max(sen, key=len)) # = 78\n",
        "wor = train['Word'].to_numpy()\n",
        "\n",
        "#b. Set the global variables\n",
        "batch_size = 32\n",
        "total_vocab = len(vocab) + 2 #17261\n",
        "sen_maxlen = sen_maxlen + (100-sen_maxlen) # make it = 100\n",
        "word_maxlen = len(max(wor, key=len)) # = 43\n",
        "\n",
        "total_sentences_train = len(train_sentences)\n",
        "total_sentences_test = len(test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwZTwBxEcz3W"
      },
      "source": [
        "'''2. Create a **matrix of sentences** where each cell is an integer representings word.\n",
        "      Each sentence is of legnth 100 (100 words).\n",
        "      Pad the sentences.\n",
        "      This matrix can be fed to the embedding layer.'''\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Convert the a sentence to an array of numbers. Add post-padding.\n",
        "def sentence_to_vector(sentence):\n",
        "  vector = []\n",
        "  for word in sentence:\n",
        "    word = word.lower()\n",
        "    if word in vocab:\n",
        "      integer_representation = vocab[word]\n",
        "      vector.append(integer_representation)\n",
        "      if len(vector) >= sen_maxlen:\n",
        "        break\n",
        "  vector = pad_sequences([vector], padding='post', maxlen=sen_maxlen)\n",
        "  return vector\n",
        "\n",
        "matrix_train_x = np.zeros((total_sentences_train, sen_maxlen))  \n",
        "for i, sentence in enumerate(train_sentences):\n",
        "  matrix_train_x[i] = sentence_to_vector(sentence)\n",
        "\n",
        "matrix_test_x = np.zeros((total_sentences_test, sen_maxlen))  \n",
        "for i, sentence in enumerate(test_sentences):\n",
        "  matrix_test_x[i] = sentence_to_vector(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaU3f6AqGzcc"
      },
      "source": [
        "'''3. Set the **embedding layer** where the rows indices correspond to the word integers.'''\n",
        "#1. Download GloVe embeddings\n",
        "\n",
        "import zipfile\n",
        "\n",
        "wget.download(\"http://nlp.stanford.edu/data/glove.6B.zip\")\n",
        "with zipfile.ZipFile(\"glove.6B.zip\", 'r') as f:\n",
        "  f.extractall(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmHRg6-86Uay",
        "outputId": "e1a57492-5576-4bfe-f0b4-7514426cb785"
      },
      "source": [
        "#2. Choose embeddings of maxlen (size 100)\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "#3. Keep minimum and maximum values in word embeddings to add\n",
        "min_emb_value = -1e-8\n",
        "max_emb_value = 1e-8\n",
        "\n",
        "#3. Make a dict mapping GloVe chars to their NumPy vector representation\n",
        "embedding_vectors = {}\n",
        "with open(path_to_glove_file, 'r', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        line_array = line.strip().split(\" \")\n",
        "        word = line_array[0]\n",
        "        vector = np.array(line_array[1:], dtype=float)\n",
        "        min_emb_value = np.min(vector) if np.min(vector)<min_emb_value else min_emb_value\n",
        "        max_emb_value = np.max(vector) if np.max(vector)>max_emb_value else max_emb_value\n",
        "        embedding_vectors[word] = vector\n",
        "\n",
        "print(f\"Found {len(embedding_vectors)} wrod vectors.\")\n",
        "\n",
        "embedding_dim = list(embedding_vectors.values())[0].shape[0]\n",
        "print(\"Embedding Vector size =\", embedding_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 wrod vectors.\n",
            "Embedding Vector size = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tDFDDRYnBTw",
        "outputId": "09fc7aed-0ab3-44d3-aab3-d652477e3386"
      },
      "source": [
        "\"min\", min_emb_value, \"max\", max_emb_value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('min', -4.0666, 'max', 3.575)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok6hdyl-m42b",
        "outputId": "6289f9de-bf63-4471-a2f8-4da33bc53949"
      },
      "source": [
        "#4. Prepare a matrix to be used in the model (embedding of words)\n",
        "misses = 0\n",
        "hits = 0\n",
        "missed_words = []\n",
        "\n",
        "word_embedding_matrix_train_x = np.random.uniform(-0.5, 0.5, size=(total_vocab, embedding_dim))  \n",
        "for word, value in vocab.items():\n",
        "  embedding_vector = embedding_vectors.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    word_embedding_matrix_train_x[value] = embedding_vector\n",
        "    hits+=1\n",
        "  else:\n",
        "    missed_words.append(word)\n",
        "    misses+=1\n",
        "\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "word_embedding_matrix_train_x.shape #added 2 for unknown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 15795 words (1464 misses)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17261, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbkh90CFmxf5",
        "outputId": "8df7bb2a-b852-4042-9a2f-5d4f4da00f91"
      },
      "source": [
        "#Example of a miss\n",
        "print(missed_words[936])\n",
        "word_embedding_matrix_train_x[vocab[missed_words[936]]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "left-stream\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.4351392 ,  0.41581059,  0.01334232, -0.11016987, -0.18609691,\n",
              "       -0.11914627, -0.04556634,  0.42029121,  0.333771  ,  0.48621899,\n",
              "       -0.12375477,  0.03143299, -0.47657092,  0.35239814,  0.03034039,\n",
              "        0.20047681,  0.04325829,  0.26232434, -0.48934633, -0.01623641,\n",
              "        0.31930768, -0.17249322, -0.0482819 , -0.40368594,  0.3063288 ,\n",
              "        0.24489311,  0.18694933,  0.27813586,  0.08740598,  0.21758698,\n",
              "        0.28123855,  0.07683449,  0.42730906,  0.32872708,  0.44066565,\n",
              "        0.08867938,  0.23198397, -0.45302301, -0.34930728, -0.47421331,\n",
              "        0.04036456,  0.44240125, -0.06515136, -0.09763807, -0.1736167 ,\n",
              "       -0.32402334, -0.46717193, -0.19608272,  0.23643656, -0.09039739,\n",
              "       -0.33710965, -0.32068986, -0.347174  ,  0.29244347, -0.29411762,\n",
              "        0.03885047,  0.36426601, -0.32662653,  0.33233578, -0.31607586,\n",
              "        0.39861772,  0.43046175,  0.27112082,  0.4780914 , -0.05850225,\n",
              "        0.18698118, -0.10932055,  0.35855302, -0.22974836, -0.16024436,\n",
              "       -0.30419614,  0.03231399,  0.02944826, -0.16049865,  0.18013301,\n",
              "       -0.08496343, -0.36357   , -0.02463406,  0.29499218,  0.48752252,\n",
              "       -0.38961436, -0.03546088, -0.42614528, -0.22565828, -0.02531719,\n",
              "        0.42300913,  0.32755754,  0.42426473, -0.08553928,  0.36365739,\n",
              "       -0.09199584, -0.34799433, -0.45759931, -0.11052695, -0.36531457,\n",
              "        0.138471  ,  0.47299375,  0.01170099,  0.38204586, -0.03730972])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPxezgq0ZCCE"
      },
      "source": [
        "#### 2. **Convert Labels to numerical vectors (one-hot-encoding)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxRjosUx7MBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad83308-13a7-4926-b41d-5856582ca8a4"
      },
      "source": [
        "# Get y labels in one-hot-encodings\n",
        "train_sentence_tags_NN = transform_to_sentence_tags_table('train_pos.txt', ohe_labels=True)['Tag']\n",
        "test_sentence_tags_NN = transform_to_sentence_tags_table('test_pos.txt', ohe_labels=True)['Tag']\n",
        "\n",
        "x_train = matrix_train_x\n",
        "y_train = train_sentence_tags_NN\n",
        "\n",
        "x_test = matrix_test_x\n",
        "y_test = np.asarray(test_sentence_tags_NN)\n",
        "\n",
        "print(\"A sentence's one-hot-encoded tags: \",y_train[0].shape)\n",
        "print(\"Number of one hot encoded tags:\", y_train[0][0].shape)\n",
        "print(x_train.shape, y_train.shape) \n",
        "\n",
        "\n",
        "#y.shape should be (sample_size, sen_maxlen, total_tags) -> Using tf.convert_to_tensor gives an error (tf.decorator). Use solution for Error #2 instead.\n",
        "#correct is: (sample_size=8936, sen_maxlen=100, total_tags=45)\n",
        "\n",
        "#Error#1: ValueError: Failed to convert a NumPy array to a Tensor (Upported object type numpy.ndarray).nsu\n",
        "#avoid by setting dtype('float64') for x and y\n",
        "\n",
        "#Error#2: ValueError: setting an array element with a sequence.\n",
        "#Comes when you do y_train.astype(np.float64). You have to iterate over each array and set its dtype to float.\n",
        "\n",
        "y_train_tf = np.array([np.asarray(p, dtype=np.float64) for p in y_train]).squeeze(1) \n",
        "print(y_train_tf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A sentence's one-hot-encoded tags:  (1, 100, 45)\n",
            "Number of one hot encoded tags: (100, 45)\n",
            "(8936, 100) (8936,)\n",
            "(8936, 100, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "L7RqL47NNCqA",
        "outputId": "ffb3334a-e54b-4e5e-d1f2-48872f0393e0"
      },
      "source": [
        "#Visualize\n",
        "numeric_train = pd.DataFrame(list(zip(x_train,y_train)), columns=['sentences', 'tags'])\n",
        "print(numeric_train['sentences'][0].shape, numeric_train['tags'][0].shape)\n",
        "numeric_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100,) (1, 100, 45)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[13779.0, 3826.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1975.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
              "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           sentences                                               tags\n",
              "0  [13779.0, 3826.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
              "1  [1975.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
              "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0...\n",
              "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
              "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  [[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIt9DTUaOOEc"
      },
      "source": [
        "#### **3. Build the model + train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20ustFsjHrN_"
      },
      "source": [
        "from keras.layers import GlobalMaxPooling1D, Flatten, Reshape, GlobalMaxPooling2D, Conv2D, Embedding, Dense, Lambda, Dropout, BatchNormalization\n",
        "from keras import Input, Model, Sequential\n",
        "\n",
        "# Create, add Embedding layer and freeze it \n",
        "embedding_layer = Embedding(\n",
        "    input_dim = total_vocab, #len(vocab)+2\n",
        "    output_dim = 100, #embedding_dim\n",
        "    # input_length=word_maxlen,\n",
        "    weights = [word_embedding_matrix_train_x],\n",
        "    trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlToUPTUvsi7"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience = 5,\n",
        "                               restore_best_weights=True\n",
        "                               )\n",
        "callbacks = [ReduceLROnPlateau(), early_stopping]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT7KksZ-vko3"
      },
      "source": [
        "#### 3.1 **NN Model with Convolutional Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67W8f9JuNCDh",
        "outputId": "6a31f2e0-6675-4c80-aae3-e4b8a4ec4204"
      },
      "source": [
        "# Get y labels in one-hot-encodings\n",
        "train_sentence_tags_NN = transform_to_sentence_tags_table('train_pos.txt', numerical_labels=True)['Tag']\n",
        "test_sentence_tags_NN = transform_to_sentence_tags_table('test_pos.txt', numerical_labels=True)['Tag']\n",
        "\n",
        "x_train_conv = matrix_train_x\n",
        "y_train_conv = train_sentence_tags_NN\n",
        "\n",
        "x_test_conv = matrix_test_x\n",
        "y_test_conv = np.asarray(test_sentence_tags_NN)\n",
        "\n",
        "print(\"A sentence's one-hot-encoded tags: \",y_train_conv[0].shape)\n",
        "print(\"Number of one hot encoded tags:\", y_train_conv[0][0].shape)\n",
        "print(x_train_conv.shape, y_train_conv.shape) \n",
        "\n",
        "\n",
        "#y.shape should be (sample_size, sen_maxlen, total_tags) -> Using tf.convert_to_tensor gives an error (tf.decorator). Use solution for Error #2 instead.\n",
        "#correct is: (sample_size=8936, sen_maxlen=100, total_tags=45)\n",
        "\n",
        "#Error#1: ValueError: Failed to convert a NumPy array to a Tensor (Upported object type numpy.ndarray).nsu\n",
        "#avoid by setting dtype('float64') for x and y\n",
        "\n",
        "#Error#2: ValueError: setting an array element with a sequence.\n",
        "#Comes when you do y_train.astype(np.float64). You have to iterate over each array and set its dtype to float.\n",
        "\n",
        "y_train_conv_tf = np.array([np.asarray(p, dtype=np.float64) for p in y_train_conv]).squeeze(1) \n",
        "print(y_train_conv_tf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A sentence's one-hot-encoded tags:  (1, 45)\n",
            "Number of one hot encoded tags: (45,)\n",
            "(8936, 100) (8936,)\n",
            "(8936, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AogRAlLDUfl-"
      },
      "source": [
        "'''Deep Learning w/ convolutional layer:\n",
        "# #1. Input as a sentence\n",
        "# #2. From sentence get embeddings with word embeddings \n",
        "# #3. Dimension of embedding matrix is 2D, so you can treat it as an image\n",
        "# #   - You can put it to Conv2D + MaxPool (to flatten after embedding) [according to paper]\n",
        "# #   - Or directly to FC \n",
        "'''\n",
        "from keras.layers import InputLayer\n",
        "\n",
        "batch_size = 64\n",
        "# input = Input(shape=(sen_maxlen,)); print(\"input\", input.shape)\n",
        "# # input = InputLayer(input_shape=(sen_maxlen, ))\n",
        "# z = embedding_layer(input); print(\"emd\", z.shape)\n",
        "# z = Reshape((sen_maxlen, embedding_dim, 1))(z); print('resh', z.shape)\n",
        "# z = Conv2D(kernel_size=5, filters=100,  activation='relu')(z); print(\"conv\", z.shape) # #filters = embedding_dim\n",
        "# z = GlobalMaxPooling2D()(z)\n",
        "# # z = BatchNormalization()(z)\n",
        "# # z = Dropout(0.2)(z)\n",
        "# # z = Flatten()(z)\n",
        "# z = Dense(196, activation='relu')(z)\n",
        "# z = BatchNormalization()(z)\n",
        "# z = Dropout(0.2)(z)\n",
        "# output = Dense(len(unique_tags),activation='softmax')(z)\n",
        "# model = Model(input, output, name='conv-POS')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(100, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Reshape((100, 100, 1)))\n",
        "model.add(Conv2D(kernel_size=5, filters=100, activation='relu'))\n",
        "model.add(GlobalMaxPooling2D())\n",
        "model.add(Dense(320, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(unique_tags),activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKyjX9aFSOlG",
        "outputId": "1df82f3f-f228-4e9d-e764-5dea25ad0070"
      },
      "source": [
        "# model.compile(optimizer=Adam(1e-2), loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1726100   \n",
            "_________________________________________________________________\n",
            "reshape_10 (Reshape)         (None, 100, 100, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 96, 96, 100)       2600      \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_9 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 320)               32320     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 320)               1280      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 45)                14445     \n",
            "=================================================================\n",
            "Total params: 1,776,745\n",
            "Trainable params: 1,776,105\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNg1GgRN6rdG"
      },
      "source": [
        "model.fit(x_train_conv,y_train_conv_tf, batch_size=batch_size, epochs=20, validation_split=0.2, callbacks=callbacks, verbose=2)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPjxzOcRv01A"
      },
      "source": [
        "#### 3.2 **RNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQYjxzIudySP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4122d302-b5bc-41b1-81d1-1771072b4f9e"
      },
      "source": [
        "#RNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(sen_maxlen, )))\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(LSTM(80, return_sequences=True)))\n",
        "model.add(Dense(len(tags_index), activation='softmax'))\n",
        "\n",
        "# \"If labels in one-hot representation, use CategoricalCrossentropy loss. \" \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1726100   \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 100, 160)          115840    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 100, 45)           7245      \n",
            "=================================================================\n",
            "Total params: 1,849,185\n",
            "Trainable params: 1,849,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDRQrxAQ3aQz",
        "outputId": "8c55b3a2-ac62-4de7-920c-96910cf2e0b1"
      },
      "source": [
        "x_train.dtype, y_train_tf.dtype '''must be float to be able to'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dtype('float64'), dtype('float64'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ9Q5afPc7SH"
      },
      "source": [
        "model.fit(x_train,y_train_tf, batch_size=64, epochs=50, validation_split=0.2, callbacks=callbacks, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hcvF-fQfKgM"
      },
      "source": [
        "###BROUILLONS\n",
        "Ignorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuVF7G4RX8kw"
      },
      "source": [
        "'''BROUILLONS'''\n",
        "# from sklearn.metrics import fbeta_score, label_ranking_average_precision_score\n",
        "# from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# # accuracy_viterbi = fbeta_score(test_y_viterbi, pred_y_viterbi,beta=0.5,  average='samples')\n",
        "# # accuracy_viterbi2 = label_ranking_average_precision_score(test_y_viterbi, pred_y_viterbi)\n",
        "\n",
        "#I got this error because funcitonal API i named output x and my training set i named x. \n",
        "# Train_x was overritten by the NN output x. which made it of type 'tensorflow.python.keras.engine.keras_tensor.KerasTensor.\n",
        "#Removing it resulted in x_train being a numpy.ndarray.\n",
        "# ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'>]\n",
        "\n",
        "\n",
        "# from tensorflow import convert_to_tensor\n",
        "# y = pad_sequences([convert_to_tensor(x) for x in y])\n",
        "# y.shape #45x45 encodings for each label. [0,0,0...] if label is not in sentence.\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# mlb = MultiLabelBinarizer(classes=label_encoder.classes_)\n",
        "# y = mlb.fit_transform(y)\n",
        "# y, y.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}